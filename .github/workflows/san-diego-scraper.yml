name: San Diego County Permit Scraper

on:
  schedule:
    # Run every Sunday at 2 AM UTC (Saturday 6 PM PST)
    - cron: '0 2 * * 0'
  workflow_dispatch: # Allow manual triggering
    inputs:
      debug_mode:
        description: 'Enable debug logging'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  WORKING_DIRECTORY: './scripts/san-diego-script'

jobs:
  scrape-san-diego:
    runs-on: ubuntu-latest
    timeout-minutes: 30 # Safety timeout for 15-minute expected runtime
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libnss3-dev \
          libatk-bridge2.0-dev \
          libdrm2 \
          libxkbcommon0 \
          libgtk-3-dev \
          libgbm-dev
          
    - name: Install Python dependencies
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        python -m pip install --upgrade pip
        pip install \
          playwright==1.40.0 \
          supabase==2.3.0 \
          pandas==2.1.4 \
          requests==2.31.0 \
          python-dotenv==1.0.0
          
    - name: Install Playwright browsers
      run: |
        playwright install chromium
        playwright install-deps chromium
        
    - name: Set up environment variables
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" >> .env
        echo "SUPABASE_ANON_KEY=${{ secrets.SUPABASE_ANON_KEY }}" >> .env
        echo "GEOCODIO_API_KEY=${{ secrets.GEOCODIO_API_KEY }}" >> .env
        echo "DEBUG_MODE=${{ github.event.inputs.debug_mode || 'false' }}" >> .env
        
    - name: Run San Diego County scraper
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        python san_diego_county_scraper.py
        
    - name: Upload scraping results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: san-diego-scraping-results-${{ github.run_number }}
        path: |
          ${{ env.WORKING_DIRECTORY }}/downloads/*.csv
          ${{ env.WORKING_DIRECTORY }}/Scraped-data-formatted/*.json
          ${{ env.WORKING_DIRECTORY }}/*.log
        retention-days: 30
        
    - name: Check scraping success
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        if [ -f "san_diego_scraper.log" ]; then
          echo "=== Scraping Log Summary ==="
          tail -20 san_diego_scraper.log
          
          # Check for success indicators
          if grep -q "Complete workflow test finished" san_diego_scraper.log; then
            echo "✅ Scraping completed successfully"
            echo "SCRAPING_STATUS=success" >> $GITHUB_ENV
          else
            echo "❌ Scraping may have failed"
            echo "SCRAPING_STATUS=failed" >> $GITHUB_ENV
            exit 1
          fi
        else
          echo "❌ No log file found"
          echo "SCRAPING_STATUS=failed" >> $GITHUB_ENV
          exit 1
        fi
        
    - name: Notify on failure
      if: failure()
      uses: actions/github-script@v7
      with:
        script: |
          const issue = await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `San Diego County Scraper Failed - ${new Date().toISOString().split('T')[0]}`,
            body: `
            ## Scraping Failure Report
            
            **Workflow Run:** [${context.runNumber}](${context.payload.repository.html_url}/actions/runs/${context.runId})
            **Triggered:** ${context.eventName}
            **Time:** ${new Date().toISOString()}
            
            Please check the workflow logs for details.
            
            ### Quick Actions:
            - [ ] Check Supabase connectivity
            - [ ] Verify API keys are valid
            - [ ] Review website structure changes
            - [ ] Check for rate limiting issues
            `,
            labels: ['scraping-failure', 'automated']
          });
          
          console.log(`Created issue #${issue.data.number}`);
